{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as psqf\n",
    "import pyspark.sql.types as psqt\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\\\n",
    "\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel, LabeledPoint\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://github.com/klrpdx/sparkify\n",
    "\n",
    "2.https://github.com/fxzero/Sparkify-Project\n",
    "\n",
    "More Options 3.https://github.com/linpingyu/Sparkify\n",
    "\n",
    "4.https://github.com/sanjeevai/sparkify-capstone\n",
    "\n",
    "5.https://github.com/gharesh/Sparkify-Project\n",
    "\n",
    "6.https://medium.com/@sandeep.nie1/sparkify-big-data-udacity-capstone-project-52be7b3c0414"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession.builder.appName(\"customer-churn data pipeline\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"mini_sparkify_event_data.json\"\n",
    "user_event = spark.read.json(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preliminary data analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+---------+------+-------------+--------------------+------+\n",
      "|          artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page| registration|sessionId|     song|status|           ts|           userAgent|userId|\n",
      "+----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+---------+------+-------------+--------------------+------+\n",
      "|  Martha Tilston|Logged In|    Colin|     M|           50| Freeman|277.89016| paid|     Bakersfield, CA|   PUT|NextSong|1538173362000|       29|Rockpools|   200|1538352117000|Mozilla/5.0 (Wind...|    30|\n",
      "|Five Iron Frenzy|Logged In|    Micah|     M|           79|    Long|236.09424| free|Boston-Cambridge-...|   PUT|NextSong|1538331630000|        8|   Canada|   200|1538352180000|\"Mozilla/5.0 (Win...|     9|\n",
      "+----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+---------+------+-------------+--------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_event.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286500 18\n"
     ]
    }
   ],
   "source": [
    "print(user_event.count(), len(user_event.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_event.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count null values for each column\n",
    "df_nulls = user_event.select([psqf.count(psqf.when(psqf.isnull(c), c)).alias(c) for c in user_event.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58392"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_event.filter(psqf.isnull(psqf.col('artist'))).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|page|\n",
      "+----+\n",
      "|   0|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's make sure page column doesn't have nulls ( we'll define churn on 'page' column)\n",
    "df_nulls.select('page').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|level|\n",
      "+-----+\n",
      "| free|\n",
      "| paid|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_event.select('level').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|level|level_count|\n",
      "+-----+-----------+\n",
      "| free|      58338|\n",
      "| paid|     228162|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "level_counts= user_event.groupby('level').agg({'level':'count'}).withColumnRenamed(\"count(level)\", \"level_count\")\n",
    "level_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|              Cancel|\n",
      "|    Submit Downgrade|\n",
      "|         Thumbs Down|\n",
      "|                Home|\n",
      "|           Downgrade|\n",
      "|         Roll Advert|\n",
      "|              Logout|\n",
      "|       Save Settings|\n",
      "|Cancellation Conf...|\n",
      "|               About|\n",
      "| Submit Registration|\n",
      "|            Settings|\n",
      "|               Login|\n",
      "|            Register|\n",
      "|     Add to Playlist|\n",
      "|          Add Friend|\n",
      "|            NextSong|\n",
      "|           Thumbs Up|\n",
      "|                Help|\n",
      "|             Upgrade|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_event.select('page').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                page|page_count|\n",
      "+--------------------+----------+\n",
      "|              Cancel|        52|\n",
      "|    Submit Downgrade|        63|\n",
      "|         Thumbs Down|      2546|\n",
      "|                Home|     14457|\n",
      "|           Downgrade|      2055|\n",
      "|         Roll Advert|      3933|\n",
      "|              Logout|      3226|\n",
      "|       Save Settings|       310|\n",
      "|Cancellation Conf...|        52|\n",
      "|               About|       924|\n",
      "| Submit Registration|         5|\n",
      "|            Settings|      1514|\n",
      "|               Login|      3241|\n",
      "|            Register|        18|\n",
      "|     Add to Playlist|      6526|\n",
      "|          Add Friend|      4277|\n",
      "|            NextSong|    228108|\n",
      "|           Thumbs Up|     12551|\n",
      "|                Help|      1726|\n",
      "|             Upgrade|       499|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "page_counts= user_event.groupby('page').agg({'page':'count'}).withColumnRenamed(\"count(page)\", \"page_count\")\n",
    "page_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at **Cancel** and **Cancellation Confirmation** samples in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------+---------+--------+-------------+---------+\n",
      "|userID|page                     |firstName|lastName|ts           |auth     |\n",
      "+------+-------------------------+---------+--------+-------------+---------+\n",
      "|18    |Cancel                   |Adriel   |Mendoza |1538943740000|Logged In|\n",
      "|18    |Cancellation Confirmation|Adriel   |Mendoza |1538943990000|Cancelled|\n",
      "|32    |Cancel                   |Diego    |Mckee   |1539033031000|Logged In|\n",
      "|32    |Cancellation Confirmation|Diego    |Mckee   |1539033046000|Cancelled|\n",
      "|125   |Cancel                   |Mason    |Hart    |1539318918000|Logged In|\n",
      "+------+-------------------------+---------+--------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cancel_events = user_event.filter(psqf.col('page').isin(['Cancel','Cancellation Confirmation'])).select(['userID','page', 'firstName', 'lastName','ts', 'auth'])\n",
    "cancel_events.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's analyse the downgrade page a bit by looking at how many of the customers who have downgraded have also cancelled thier subscriptions. The registrationId uniquely identifies each customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 52\n"
     ]
    }
   ],
   "source": [
    "cancel_reg_ids  = [vv['userID'] for vv in cancel_events.select('userID').collect()]\n",
    "print(len(cancel_reg_ids), len(set(cancel_reg_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+--------+-------------+---------+\n",
      "|userID|page     |firstName|lastName|ts           |auth     |\n",
      "+------+---------+---------+--------+-------------+---------+\n",
      "|54    |Downgrade|Alexi    |Warren  |1538354749000|Logged In|\n",
      "|95    |Downgrade|Faigy    |Howe    |1538373286000|Logged In|\n",
      "|95    |Downgrade|Faigy    |Howe    |1538392612000|Logged In|\n",
      "|131   |Downgrade|Kael     |Baker   |1538393618000|Logged In|\n",
      "|95    |Downgrade|Faigy    |Howe    |1538393664000|Logged In|\n",
      "+------+---------+---------+--------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#customer who downgraded\n",
    "downgrade_events = user_event.filter(psqf.col('page').isin(['Downgrade'])).select(['userID','page', 'firstName', 'lastName','ts', 'auth'])\n",
    "downgrade_events.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2055 154\n"
     ]
    }
   ],
   "source": [
    "downgrade_reg_ids = [vv['userID'] for vv in downgrade_events.select('userID').collect()]\n",
    "print(len(downgrade_reg_ids), len(set(downgrade_reg_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.73% of customers who downgraded have also cancelled their subscriptions\n"
     ]
    }
   ],
   "source": [
    "# Now let's see which of those who downgraded also cancel thier subscription\n",
    "down_cancel = set(cancel_reg_ids).intersection((set(downgrade_reg_ids)))\n",
    "print('{0:.2f}% of customers who downgraded have also cancelled their subscriptions'.format(\n",
    "    100*(len(down_cancel))/len(set(downgrade_reg_ids))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'18'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(down_cancel)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+-------------+\n",
      "|userID|     page|firstName|           ts|\n",
      "+------+---------+---------+-------------+\n",
      "|    18|Downgrade|   Adriel|1538749555000|\n",
      "|    18|Downgrade|   Adriel|1538935253000|\n",
      "|    18|Downgrade|   Adriel|1538943739000|\n",
      "|    18|   Cancel|   Adriel|1538943740000|\n",
      "+------+---------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_event.filter((psqf.col('userID') == list(down_cancel)[0]) &\n",
    "                  (psqf.col('page').isin(['Downgrade','Cancel']))).select(['userID','page', 'firstName','ts']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inorder to understand the sequence of events, let's convert the timestamp into datatime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_udf = psqf.udf(lambda x: datetime.datetime.utcfromtimestamp(x/1000.0).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_event = user_event.select('*').withColumn('ts_datetime', ts_udf(psqf.col('ts')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------+---------+-------------+-------------------+\n",
      "|userID|page                     |firstName|ts           |ts_datetime        |\n",
      "+------+-------------------------+---------+-------------+-------------------+\n",
      "|100023|Downgrade                |Sawyer   |1538785799000|2018-10-06 00:29:59|\n",
      "|100023|Downgrade                |Sawyer   |1538963916000|2018-10-08 01:58:36|\n",
      "|100023|Downgrade                |Sawyer   |1539010201000|2018-10-08 14:50:01|\n",
      "|100023|Downgrade                |Sawyer   |1539011530000|2018-10-08 15:12:10|\n",
      "|100023|Downgrade                |Sawyer   |1539024088000|2018-10-08 18:41:28|\n",
      "|100023|Downgrade                |Sawyer   |1539040537000|2018-10-08 23:15:37|\n",
      "|100023|Downgrade                |Sawyer   |1539299163000|2018-10-11 23:06:03|\n",
      "|100023|Downgrade                |Sawyer   |1539303978000|2018-10-12 00:26:18|\n",
      "|100023|Downgrade                |Sawyer   |1539475171000|2018-10-13 23:59:31|\n",
      "|100023|Cancel                   |Sawyer   |1539475172000|2018-10-13 23:59:32|\n",
      "|100023|Cancellation Confirmation|Sawyer   |1539475237000|2018-10-14 00:00:37|\n",
      "+------+-------------------------+---------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_event.filter((psqf.col('userID') == list(down_cancel)[1]) &\n",
    "                  (psqf.col('page').isin(['Downgrade','Cancel', 'Cancellation Confirmation']))).select(['userID','page', 'firstName','ts', 'ts_datetime']).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loooking at just on customer, we can see that he was the downgrade page and cancelled after 1 hour.You are welcome to see some of the other users to see this is the case. Here I think the number of Downgrades a user completed could be an indication that user will terminate subscription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Churn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above analysis shows that **Cancel** is followed by **cancellation confirmation page** and both indicate user is canceling thier subscreption unless they change thier mind once they are on the **cancel** page which seems unlikely event at least for this dataset. So we will use both of these events to define Churn. We have also seen that about 23% of the customers who downgraded have also cancelled thier subscription. However, we'll exclude those who downgraded but didn't cancel yet for this analysis even thought they seem candidates who will eventually cancel their subscription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_event = user_event.groupby('userId').agg(psqf.collect_list('page').alias('pages'))\n",
    "# define 1 as churned, 0 otherwise\n",
    "churn_f = psqf.udf(lambda x: 1 if 'Cancel' in set(x) else 0)\n",
    "churn_event = churn_event.withColumn(\"churned\", churn_f(churn_event.pages)).drop('pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|userId|churned|\n",
      "+------+-------+\n",
      "|100010|      0|\n",
      "|200002|      0|\n",
      "|   125|      1|\n",
      "|   124|      0|\n",
      "|    51|      1|\n",
      "|     7|      0|\n",
      "|    15|      0|\n",
      "|    54|      1|\n",
      "|   155|      0|\n",
      "|100014|      1|\n",
      "|   132|      0|\n",
      "|   154|      0|\n",
      "|   101|      1|\n",
      "|    11|      0|\n",
      "|   138|      0|\n",
      "|300017|      0|\n",
      "|100021|      1|\n",
      "|    29|      1|\n",
      "|    69|      0|\n",
      "|   112|      0|\n",
      "+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "churn_event.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove features that are not useful for our analysis\n",
    "clean_df = user_event.select('artist','auth','firstName','gender','lastName','length','level','location','page','song','ts','userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df  = churn_event.join(clean_df, 'userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+---------+---------+------+---------+---------+-----+--------------------+---------+--------------------+-------------+\n",
      "|userId|churned|              artist|     auth|firstName|gender| lastName|   length|level|            location|     page|                song|           ts|\n",
      "+------+-------+--------------------+---------+---------+------+---------+---------+-----+--------------------+---------+--------------------+-------------+\n",
      "|100010|      0|Sleeping With Sirens|Logged In| Darianna|     F|Carpenter|202.97098| free|Bridgeport-Stamfo...| NextSong|Captain Tyin Knot...|1539003534000|\n",
      "|100010|      0|Francesca Battist...|Logged In| Darianna|     F|Carpenter|196.54485| free|Bridgeport-Stamfo...| NextSong|Beautiful_ Beauti...|1539003736000|\n",
      "|100010|      0|              Brutha|Logged In| Darianna|     F|Carpenter|263.13098| free|Bridgeport-Stamfo...| NextSong|          She's Gone|1539003932000|\n",
      "|100010|      0|                null|Logged In| Darianna|     F|Carpenter|     null| free|Bridgeport-Stamfo...|Thumbs Up|                null|1539003933000|\n",
      "|100010|      0|         Josh Ritter|Logged In| Darianna|     F|Carpenter|316.23791| free|Bridgeport-Stamfo...| NextSong|      Folk Bloodbath|1539004195000|\n",
      "+------+-------+--------------------+---------+---------+------+---------+---------+-----+--------------------+---------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labeled_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+---------+------+--------+------+-----+--------------------+------+----+-------------+\n",
      "|userId|churned|artist|     auth|firstName|gender|lastName|length|level|            location|  page|song|           ts|\n",
      "+------+-------+------+---------+---------+------+--------+------+-----+--------------------+------+----+-------------+\n",
      "|   125|      1|  null|Logged In|    Mason|     M|    Hart|  null| free|  Corpus Christi, TX|Cancel|null|1539318918000|\n",
      "|    51|      1|  null|Logged In|    Ethan|     M| Johnson|  null| paid|Lexington-Fayette...|Cancel|null|1539761830000|\n",
      "|    54|      1|  null|Logged In|    Alexi|     F|  Warren|  null| paid|Spokane-Spokane V...|Cancel|null|1542051577000|\n",
      "|100014|      1|  null|Logged In|  Rodrigo|     M|   Carey|  null| paid|New York-Newark-J...|Cancel|null|1542740642000|\n",
      "|   101|      1|  null|Logged In|     Alex|     M|   Hogan|  null| paid|Denver-Aurora-Lak...|Cancel|null|1539728810000|\n",
      "+------+-------+------+---------+---------+------+--------+------+-----+--------------------+------+----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labeled_df.filter(psqf.col('page') ==\"Cancel\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|Churned|count(churned)|\n",
      "+-------+--------------+\n",
      "|      0|        241636|\n",
      "|      1|         44864|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labeled_df.groupby(\"Churned\").agg({'churned': 'count'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which of those features can be useful to define churn?\n",
    "- ts: we can get information like total number of hours a customer played songs\n",
    "- song: we can get the number of songs a customer played\n",
    "- from **page** column we can find the number of ThumbsUp, ThumbsDown, and Downgrades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of songs each user played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|userId|SongsPlayed|\n",
      "+------+-----------+\n",
      "|    10|        673|\n",
      "|   100|       2682|\n",
      "|100001|        133|\n",
      "|100002|        195|\n",
      "|100003|         51|\n",
      "+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songsplayed = labeled_df.where(psqf.col('song')!='null').groupby(\"userId\").agg(psqf.count(psqf.col('song')).alias('SongsPlayed')).orderBy('userId')\n",
    "songsplayed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of distinct hour counts  a user logged in the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-------------+\n",
      "|userId|           ts|         hour|\n",
      "+------+-------------+-------------+\n",
      "|100010|1539003534000|2018-10-08-12|\n",
      "|100010|1539003736000|2018-10-08-13|\n",
      "|100010|1539003932000|2018-10-08-13|\n",
      "|100010|1539003933000|2018-10-08-13|\n",
      "|100010|1539004195000|2018-10-08-13|\n",
      "+------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hours_udf = psqf.udf(lambda x: datetime.datetime.utcfromtimestamp(x/1000.0).strftime('%Y-%m-%d-%H'))\n",
    "hours_df  = labeled_df.select('userId', 'ts').withColumn('hour', hours_udf(psqf.col('ts')))\n",
    "hours_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|userId|HourCount|\n",
      "+------+---------+\n",
      "|      |     1306|\n",
      "|    10|       51|\n",
      "|   100|      218|\n",
      "|100001|       14|\n",
      "|100002|       18|\n",
      "+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hour_count_df = hours_df.where(psqf.col('userId')!='null').groupby('userId').agg((psqf.countDistinct(psqf.col('hour'))).alias(\"HourCount\")).orderBy('userId')\n",
    "hour_count_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thumbs Up and Thumbs Down counts:\n",
    "A user having a lot of thumbs down could be an indication of the users disastisfaction with song recommendations from Sparkify while a more thumbs up ( likes) by a user indicates user is happy with song recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out rows with userId == null\n",
    "labeled_df = labeled_df.where(psqf.col('user')!='null')\n",
    "labeled_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|userId|thumbsUpCount|\n",
      "+------+-------------+\n",
      "|10    |37           |\n",
      "|100   |148          |\n",
      "|100001|8            |\n",
      "|100002|5            |\n",
      "|100003|3            |\n",
      "+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "thumbsup_count = labeled_df.where((psqf.col('page')=='Thumbs Up') &(psqf.col('userId')!='null')).groupby(\"userId\").agg(psqf.count(psqf.col('page')).alias('thumbsUpCount')).orderBy('userId')\n",
    "thumbsup_count.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|userId|thumbsDownCount|\n",
      "+------+---------------+\n",
      "|    10|              4|\n",
      "|   100|             27|\n",
      "|100001|              2|\n",
      "|100004|             11|\n",
      "|100005|              3|\n",
      "+------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "thumbsdown_count = labeled_df.where((psqf.col(\"page\")=='Thumbs Down')&(psqf.col('userId')!='null')).groupby(\"userId\").agg(psqf.count(psqf.col('page')).alias('thumbsDownCount')).orderBy('userId')\n",
    "thumbsdown_count.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join all the features\n",
    "features_df = churn_event.join(songsplayed, \"userId\").\\\n",
    "join(hour_count_df, \"userId\").join(thumbsup_count, \"userId\").join(thumbsdown_count, \"userId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----------+---------+-------------+---------------+\n",
      "|userId|churned|SongsPlayed|HourCount|thumbsUpCount|thumbsDownCount|\n",
      "+------+-------+-----------+---------+-------------+---------------+\n",
      "|100010|      0|        275|       26|           17|              5|\n",
      "|200002|      0|        387|       32|           21|              6|\n",
      "|   124|      0|       4079|      306|          171|             41|\n",
      "|    51|      1|       2111|      156|          100|             21|\n",
      "|     7|      0|        150|       18|            7|              1|\n",
      "+------+-------+-----------+---------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|churned|            features|\n",
      "+-------+--------------------+\n",
      "|      0|[275.0,26.0,17.0,...|\n",
      "|      0|[387.0,32.0,21.0,...|\n",
      "|      0|[4079.0,306.0,171...|\n",
      "|      1|[2111.0,156.0,100...|\n",
      "+-------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"SongsPlayed\", \"HourCount\", \"thumbsUpCount\", \"thumbsDownCount\"], outputCol=\"features\")\n",
    "features_df = assembler.transform(features_df)\n",
    "features_df.select('churned', 'features').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True)\n",
    "scalerModel = scaler.fit(features_df)\n",
    "features_df = scalerModel.transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------------------------------------------------------+\n",
      "|churned|scaledFeatures                                                                 |\n",
      "+-------+-------------------------------------------------------------------------------+\n",
      "|0      |[0.2463999205714785,0.2874672791008265,0.2554360395763283,0.3786242371997744]  |\n",
      "|0      |[0.34675188822240793,0.353805881970248,0.3155386371236997,0.45434908463972923] |\n",
      "|0      |[3.654782821858403,3.3832687463404962,2.569386045150126,3.10471874503815]      |\n",
      "|1      |[1.891455390277786,1.7248036746049589,1.5025649386842843,1.5902217962390524]   |\n",
      "|0      |[0.1343999566753519,0.19901580860826448,0.1051795457078999,0.07572484743995488]|\n",
      "+-------+-------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df.select('churned', 'scaledFeatures').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|      scaledFeatures|churned|\n",
      "+--------------------+-------+\n",
      "|[0.24639992057147...|      0|\n",
      "|[0.34675188822240...|      0|\n",
      "+--------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_data = features_df.select('scaledFeatures', 'churned')\n",
    "input_data.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled Points\n",
    "MLlib requires that our features be expressed with LabeledPoints. The required format for a labeled point is a tuple of the response value and a vector of predictors. We can call 'map' on df in order to return an RDD of LabeledPoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.246399920571,0.287467279101,0.255436039576,0.3786242372]),\n",
       " LabeledPoint(0.0, [0.346751888222,0.35380588197,0.315538637124,0.45434908464]),\n",
       " LabeledPoint(0.0, [3.65478282186,3.38326874634,2.56938604515,3.10471874504]),\n",
       " LabeledPoint(1.0, [1.89145539028,1.7248036746,1.50256493868,1.59022179624]),\n",
       " LabeledPoint(0.0, [0.134399956675,0.199015808608,0.105179545708,0.07572484744])]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_rdd = input_data.rdd.map(lambda line:LabeledPoint(line[-1],[line[:-1]]))\n",
    "input_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split datest into train, validation and test sets\n",
    "- train model using training dataset\n",
    "- tune model using validation dataset\n",
    "- test best model selected based on validation dataset on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (since dataset is imbalanced)\n",
    "(trainingData, tempData) = input_rdd.randomSplit([0.6, 0.4])\n",
    "(validationData, testData) = tempData.randomSplit([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=3, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=4, maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.1875\n",
      "Learned classification forest model:\n",
      "TreeEnsembleModel classifier with 3 trees\n",
      "\n",
      "  Tree 0:\n",
      "    If (feature 1 <= 0.07186681977520662)\n",
      "     Predict: 1.0\n",
      "    Else (feature 1 > 0.07186681977520662)\n",
      "     If (feature 3 <= 0.34076181347979695)\n",
      "      If (feature 2 <= 0.30802581243027827)\n",
      "       If (feature 0 <= 0.23743992345978837)\n",
      "        Predict: 0.0\n",
      "       Else (feature 0 > 0.23743992345978837)\n",
      "        Predict: 1.0\n",
      "      Else (feature 2 > 0.30802581243027827)\n",
      "       Predict: 0.0\n",
      "     Else (feature 3 > 0.34076181347979695)\n",
      "      If (feature 1 <= 0.9342686570776861)\n",
      "       If (feature 0 <= 0.6003198064832386)\n",
      "        Predict: 1.0\n",
      "       Else (feature 0 > 0.6003198064832386)\n",
      "        Predict: 0.0\n",
      "      Else (feature 1 > 0.9342686570776861)\n",
      "       Predict: 0.0\n",
      "  Tree 1:\n",
      "    If (feature 0 <= 0.057343981514816816)\n",
      "     Predict: 1.0\n",
      "    Else (feature 0 > 0.057343981514816816)\n",
      "     If (feature 2 <= 2.0960780894645765)\n",
      "      If (feature 3 <= 1.7416714911189621)\n",
      "       If (feature 3 <= 1.62808421995903)\n",
      "        Predict: 0.0\n",
      "       Else (feature 3 > 1.62808421995903)\n",
      "        Predict: 1.0\n",
      "      Else (feature 3 > 1.7416714911189621)\n",
      "       If (feature 1 <= 0.6412731610710745)\n",
      "        Predict: 1.0\n",
      "       Else (feature 1 > 0.6412731610710745)\n",
      "        Predict: 0.0\n",
      "     Else (feature 2 > 2.0960780894645765)\n",
      "      If (feature 0 <= 3.015935027794897)\n",
      "       If (feature 3 <= 1.2494599827592554)\n",
      "        Predict: 0.0\n",
      "       Else (feature 3 > 1.2494599827592554)\n",
      "        Predict: 1.0\n",
      "      Else (feature 0 > 3.015935027794897)\n",
      "       Predict: 0.0\n",
      "  Tree 2:\n",
      "    If (feature 2 <= 0.03756412346710711)\n",
      "     Predict: 1.0\n",
      "    Else (feature 2 > 0.03756412346710711)\n",
      "     If (feature 3 <= 0.34076181347979695)\n",
      "      Predict: 0.0\n",
      "     Else (feature 3 > 0.34076181347979695)\n",
      "      If (feature 3 <= 1.7416714911189621)\n",
      "       If (feature 1 <= 1.630823987206612)\n",
      "        Predict: 0.0\n",
      "       Else (feature 1 > 1.630823987206612)\n",
      "        Predict: 1.0\n",
      "      Else (feature 3 > 1.7416714911189621)\n",
      "       Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(validationData.map(lambda x: x.features))\n",
    "labelsAndPredictions = validationData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification forest model:')\n",
    "print(model.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
